{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the computation time for LJ Pot\n",
    "\n",
    "This jupyter notebook is intented to test the computation time of \n",
    "different implementations for the Lennard Jones potential with an  \n",
    "S1 switch function. Generally it should also be able to handle particle types.  \n",
    "Remember that the implementation of the switch function for particle  \n",
    "types used here is unnecessarily complicated.  \n",
    "The different implementations also differ by more than 1 percent at times.  \n",
    "Also, this is the \"loosing your loops\" talk on vectorization in numpy  \n",
    "I wanted to share, giving a good overview:  \n",
    "https://www.youtube.com/watch?v=EEUXKG97YRw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from numba import njit, jit\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "configuration = np.random.uniform(0, 4, (N, 3))\n",
    "sigma = np.array([1] * N)\n",
    "sigma_arr = 0.5 * (sigma[:, None] + sigma)\n",
    "sigma_arr_6 = sigma_arr**6\n",
    "sigma_arr_12 = sigma_arr**12\n",
    "epsilon = np.array([1] * N)\n",
    "epsilon_arr = np.sqrt(epsilon[:, None] * epsilon)\n",
    "cutoff_lj = 3.5\n",
    "switch_width_lj = 1\n",
    "switch_start_lj = 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loops\n",
    "\n",
    "simple implementation with loops  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lj_potential_pairwise(distance, sigma_lj, epsilon_lj):\n",
    "    if(distance <= 0 or distance > cutoff_lj):\n",
    "        return 0.\n",
    "    else:\n",
    "        phi_LJ = 4. * epsilon_lj * sigma_lj**6 * (sigma_lj**6 / distance**12 - 1 / distance**6)\n",
    "        if(distance <= cutoff_lj - switch_width_lj):\n",
    "            return phi_LJ\n",
    "        else:\n",
    "            t = (distance - cutoff_lj) / switch_width_lj\n",
    "            switch = t * t * (3. + 2. * t)\n",
    "            return phi_LJ * switch\n",
    "        \n",
    "def lj_potential_loops(x, sigma_arr, epsilon_arr):\n",
    "    output = np.zeros(len(x))\n",
    "    for i in range(len(x)):\n",
    "        potential = 0\n",
    "        for j in range(i, len(x)):\n",
    "            sigma_lj = sigma_arr[i, j]\n",
    "            epsilon_lj = epsilon_arr[i, j]\n",
    "            distance = np.linalg.norm(x[i, :] - x[j, :])\n",
    "            potential += lj_potential_pairwise(distance,sigma_lj, epsilon_lj)\n",
    "        output[i] = potential\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f lj_potential_loops lj_potential_loops(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing_loops = %timeit lj_potential_loops(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy\n",
    "Using np.apply_along_axis, which allows for convienient implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def potential_along_axis(x):\n",
    "        # potenital w/o switch\n",
    "        if x[0] > 0 and x[0] <= cutoff_lj - switch_width_lj:\n",
    "            x[0] = 4 * x[2] * x[1]**6 * (x[1]**6 / x[0]**12 - 1 / x[0]**6)\n",
    "            \n",
    "        # potential with switch\n",
    "        elif x[0] > cutoff_lj - switch_width_lj and x[0] <= cutoff_lj:\n",
    "            t = (x[0] - cutoff_lj) / (switch_width_lj)\n",
    "            switch = 2 * t ** 3 + 3 * t ** 2\n",
    "            x[0] = switch * (4 * x[2] * x[1]**6 * (x[1]**6 / x[0]**12 - 1 / x[0]**6))\n",
    "            \n",
    "        # potential after cutoff\n",
    "        elif x[0] > cutoff_lj:\n",
    "            x[0] = 0\n",
    "        return x[0]\n",
    "\n",
    "def lj_potential_np_along_axis(x, sigma_arr, epsilon_arr):\n",
    "    # initialize output as array with distances and corresponding sigma, epsilon along axis=2\n",
    "    output = np.zeros((x.shape[0], x.shape[0], 3))\n",
    "    output[:, :, 0] = np.linalg.norm(x[:, None, :] - x[None, :, :], axis=-1)\n",
    "    output[:, :, 1] = sigma_arr\n",
    "    output[:, :, 2] = epsilon_arr\n",
    "    \n",
    "    # calculate potentials\n",
    "    output[:, :, 0] = np.apply_along_axis(potential_along_axis, 2, output)\n",
    "    output = np.sum(output[:, :, 0], axis=-1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(0, 4, (100, 3))\n",
    "sigma_temp = np.array([1] * len(x))\n",
    "sigma_arr_temp= 0.5 * (sigma_temp[:, None] + sigma_temp)\n",
    "epsilon_temp = np.array([1] * len(x))\n",
    "epsilon_arr_temp = np.sqrt(epsilon_temp[:, None] * epsilon_temp)\n",
    "\n",
    "np.testing.assert_allclose(lj_potential_np_along_axis(x, sigma_arr_temp, epsilon_arr_temp), \n",
    "                          lj_potential_loops(x, sigma_arr_temp, epsilon_arr_temp), rtol=1E-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f lj_potential_np_along_axis lj_potential_np_along_axis(configuration, sigma_arr, epsilon_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timings_np_apply_along_axis = %timeit lj_potential_np_along_axis(configuration, sigma_arr, epsilon_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# turns out its slow\n",
    "Replace apply_along_axis with a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def potential_along_axis(x):\n",
    "        # potenital w/o switch\n",
    "        if x[0] > 0 and x[0] <= cutoff_lj - switch_width_lj:\n",
    "            x[0] = 4 * x[2] * x[1]**6 * (x[1]**6 / x[0]**12 - 1 / x[0]**6)\n",
    "            \n",
    "        # potential with switch\n",
    "        elif x[0] > cutoff_lj - switch_width_lj and x[0] <= cutoff_lj:\n",
    "            t = (x[0] - cutoff_lj) / (cutoff_lj - cutoff_lj - switch_width_lj)\n",
    "            switch = 2 * t ** 3 + 3 * t ** 2\n",
    "            x[0] = switch * (4 * x[2] * x[1]**6 * (x[1]**6 / x[0]**12 - 1 / x[0]**6))\n",
    "            \n",
    "        # potential after cutoff\n",
    "        elif x[0] > cutoff_lj:\n",
    "            x[0] = 0\n",
    "        return x[0]\n",
    "\n",
    "def lj_potential_np_along_axis_replace_loop(x, sigma_arr, epsilon_arr):\n",
    "    # initialize output as array with distances and corresponding sigma, epsilon along axis=2\n",
    "    output = np.zeros((x.shape[0], x.shape[0], 3))\n",
    "    output[:, :, 0] = np.linalg.norm(x[:, None, :] - x[None, :, :], axis=-1)\n",
    "    output[:, :, 1] = sigma_arr\n",
    "    output[:, :, 2] = epsilon_arr\n",
    "    \n",
    "    # calculate potentials\n",
    "    for i in range(len(output)):\n",
    "        for j in range(i, len(output)):\n",
    "            output[i, j, 0] = potential_along_axis(output[i, j, :])\n",
    "    output = np.sum(output[:, :, 0], axis=-1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(0, 4, (100, 3))\n",
    "sigma_temp = np.array([1] * len(x))\n",
    "sigma_arr_temp= 0.5 * (sigma_temp[:, None] + sigma_temp)\n",
    "epsilon_temp = np.array([1] * len(x))\n",
    "epsilon_arr_temp = np.sqrt(epsilon_temp[:, None] * epsilon_temp)\n",
    "\n",
    "np.testing.assert_allclose(lj_potential_np_along_axis_replace_loop(x, sigma_arr_temp, epsilon_arr_temp), \n",
    "                           lj_potential_loops(x, sigma_arr_temp, epsilon_arr_temp), rtol=1E-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f lj_potential_np_along_axis_replace_loop lj_potential_np_along_axis_replace_loop(configuration, sigma_arr, epsilon_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timings_np_apply_along_axis_replace_loop = %timeit lj_potential_np_along_axis_replace_loop(configuration, sigma_arr, epsilon_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy.piecewise\n",
    "\n",
    "Allows for somewhat convenient implementation but does not handle particle types easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lj_potential_np_piecewise(x, sigma, epsilon):\n",
    "        distances = np.linalg.norm(x[:, None, :] - x[None, :, :], axis=-1)\n",
    "\n",
    "        # potential_pairwise\n",
    "        def p1(d):\n",
    "            sigma6 = sigma ** 6\n",
    "            potential = 4 * epsilon * sigma6 * (sigma6 / d ** 12 - 1 / d ** 6)\n",
    "            return potential\n",
    "\n",
    "        # potential_pairwise with switch function smoothstep S1\n",
    "        def p2(d):\n",
    "            t = (d - cutoff_lj) / (cutoff_lj - switch_start_lj)\n",
    "            switch_function = t * t * (3. + 2. * t)\n",
    "            sigma6 = sigma ** 6\n",
    "            potential = 4 * epsilon * sigma6 * (sigma6 / d ** 12 - 1 / d ** 6)\n",
    "            return potential * switch_function\n",
    "\n",
    "        # piecewise function for Lennard Jones Potential\n",
    "        def p12(d):\n",
    "            output = np.piecewise(d, [d <= 0,\n",
    "                                 (0 < d) & (d < switch_start_lj),\n",
    "                                 (switch_start_lj <= d) & (d < cutoff_lj),\n",
    "                                 cutoff_lj <= d],\n",
    "                                 [0, p1, p2,0]\n",
    "                                 )\n",
    "            return output\n",
    "        \n",
    "         # sum potentials for every particle\n",
    "        potential = np.sum(p12(distances), axis=-1)\n",
    "        return potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_temp = 1\n",
    "epsilon_temp = 1\n",
    "x = np.random.uniform(0, 4, (100, 3))\n",
    "np.testing.assert_allclose(lj_potential_np_piecewise(x, sigma_temp, epsilon_temp),\n",
    "                           lj_potential_loops(x, sigma_arr_temp, epsilon_arr_temp), rtol=1E-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f lj_potential_np_piecewise lj_potential_np_piecewise(configuration, sigma_temp, epsilon_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timings_np_piecewise = %timeit lj_potential_np_piecewise(configuration, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def lj_potential_numpy(x, sigma_arr, sigma_6_arr, sigma_12_arr, epsilon_arr):\n",
    "    output = np.zeros(len(x))\n",
    "    \n",
    "    # get distances\n",
    "    d = np.linalg.norm(x[:, None, :] - x[None, :, :], axis=-1)\n",
    "    \n",
    "    # get masks\n",
    "    mask1 = (d > switch_start_lj * sigma_arr) | (d <= 0)\n",
    "    mask2 = (d < switch_start_lj * sigma_arr) | (d > cutoff_lj * sigma_arr)\n",
    "    \n",
    "    # mask distances and arrays for mixed constants\n",
    "    d_masked1 = np.ma.masked_where(mask1, d)\n",
    "    d_masked2 = np.ma.masked_where(mask2, d)\n",
    "    sigma_arr_6_masked1 = np.ma.masked_where(mask1, sigma_6_arr)\n",
    "    sigma_arr_12_masked1 = np.ma.masked_where(mask1, sigma_12_arr)\n",
    "    sigma_arr_6_masked2 = np.ma.masked_where(mask2, sigma_6_arr)\n",
    "    sigma_arr_12_masked2 = np.ma.masked_where(mask2, sigma_12_arr)\n",
    "    epsilon_arr_masked1 = np.ma.masked_where(mask1, epsilon_arr)\n",
    "    epsilon_arr_masked2 = np.ma.masked_where(mask2, epsilon_arr)\n",
    "    \n",
    "    # calculate potential\n",
    "    t = (d_masked2 - cutoff_lj) / (cutoff_lj - switch_start_lj)\n",
    "    switch = t * t * (3. + 2. * t)\n",
    "    output = (np.array(4 * epsilon_arr_masked1 \n",
    "             * (sigma_arr_12_masked1 / d_masked1**12 - sigma_arr_6_masked1 / d_masked1**6))\n",
    "             + np.array(4 * epsilon_arr_masked2 * switch \n",
    "             * (sigma_arr_12_masked2 / d_masked2**12 - sigma_arr_6_masked2 / d_masked2**6)))\n",
    "    return np.sum(output, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(0, 4, (100, 3))\n",
    "sigma_temp = np.array([1] * len(x))\n",
    "sigma_arr_temp = 0.5 * (sigma_temp[:, None] + sigma_temp)\n",
    "sigma_arr_6_temp = sigma_arr_temp**6\n",
    "sigma_arr_12_temp = sigma_arr_temp**12\n",
    "epsilon_temp = np.array([1] * len(x))\n",
    "epsilon_arr_temp = np.sqrt(epsilon_temp[:, None] * epsilon_temp)\n",
    "\n",
    "np.testing.assert_allclose(lj_potential_numpy(x, sigma_arr_temp, sigma_arr_6_temp, sigma_arr_12_temp, \n",
    "                           epsilon_arr_temp), lj_potential_loops(x, sigma_arr_temp, epsilon_arr_temp), rtol=1E-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f lj_potential_numpy lj_potential_numpy(configuration, sigma_arr, sigma_arr_6, sigma_arr_12, epsilon_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing_numpy = %timeit lj_potential_numpy(configuration, sigma_arr, sigma_arr_6, sigma_arr_12, epsilon_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out Numpy does spread work on multiple cores by default. At any given time 1.5 of my total 4 logical cores where being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lj_potential_pytorch(x, sigma_arr, sigma_6_arr, sigma_12_arr, epsilon_arr):\n",
    "    # get distances, init tensors\n",
    "    x = torch.Tensor(x).double()\n",
    "    d = torch.norm(x[:, None, :] - x[None, :, :], dim=-1)\n",
    "    sigma_arr = torch.Tensor(sigma_arr).double()\n",
    "    sigma_6_arr = torch.Tensor(sigma_6_arr).double()\n",
    "    epsilon_arr = torch.Tensor(epsilon_arr).double()\n",
    "    # get masks\n",
    "    mask1 = ((d > 0) & (d < 2.5 * sigma_arr))\n",
    "    mask2 = ((d > 2.5 * sigma_arr) & (d < 3.5 * sigma_arr))\n",
    "    # init output and caculate potential\n",
    "    output = torch.tensor((), dtype=torch.float64).new_zeros(d.size())\n",
    "    output[mask1] = (4 * epsilon_arr[mask1] * sigma_6_arr[mask1] \n",
    "                     * (sigma_6_arr[mask1] / d[mask1]**12 - sigma_6_arr[mask1] / d[mask1]**6))\n",
    "    output[mask2] = (4 * epsilon_arr[mask2] * sigma_6_arr[mask2] \n",
    "                     * (sigma_6_arr[mask2] / d[mask2]**12 - sigma_6_arr[mask2] / d[mask2]**6)\n",
    "                     * 3 * ((d[mask2] - cutoff_lj) / (cutoff_lj - switch_start_lj))**2\n",
    "                     * 2 * ((d[mask2] - cutoff_lj) / (cutoff_lj - switch_start_lj))**3)\n",
    "    output = torch.sum(output, dim=-1)\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, pytroch uses the same core at 100% by default. I quick google search did not yield a ways to use multiple cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(0, 4, (100, 3))\n",
    "sigma_temp = np.array([1] * len(x))\n",
    "sigma_arr_temp = 0.5 * (sigma_temp[:, None] + sigma_temp)\n",
    "sigma_arr_6_temp = sigma_arr_temp**6\n",
    "sigma_arr_12_temp = sigma_arr_temp**12\n",
    "epsilon_temp = np.array([1] * len(x))\n",
    "epsilon_arr_temp = np.sqrt(epsilon_temp[:, None] * epsilon_temp)\n",
    "\n",
    "np.testing.assert_allclose(lj_potential_pytorch(x, sigma_arr_temp, sigma_arr_6_temp, sigma_arr_12_temp, \n",
    "                           epsilon_arr_temp), lj_potential_loops(x, sigma_arr_temp, epsilon_arr_temp), rtol=1E-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f lj_potential_pytorch lj_potential_pytorch(configuration, sigma_arr, sigma_arr_6, sigma_arr_12, epsilon_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing_pytorch = %timeit lj_potential_pytorch(configuration, sigma_arr, sigma_arr_6, sigma_arr_12, epsilon_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try it with CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lj_potential_pytorch_cuda(x, sigma, epsilon):\n",
    "    # get distances, init tensors\n",
    "    x = torch.Tensor(x).double().cuda()\n",
    "    d = torch.norm(x[:, None, :] - x[None, :, :], dim=-1).cuda()\n",
    "    sigma = torch.Tensor(sigma).double().cuda()\n",
    "    sigma_arr = 0.5 * (sigma[:, None] + sigma)\n",
    "    sigma_arr_6 = sigma_arr**6\n",
    "    epsilon = torch.Tensor(epsilon).double().cuda()\n",
    "    epsilon_arr = torch.sqrt(epsilon[:, None] * epsilon)\n",
    "    \n",
    "    # get masks\n",
    "    mask1 = ((d > 0) & (d < 2.5 * sigma_arr)).cuda()\n",
    "    \n",
    "    mask2 = ((d > 2.5 * sigma_arr) & (d < 3.5 * sigma_arr)).cuda()\n",
    "    \n",
    "    # init output and caculate potential\n",
    "    output = torch.tensor((), dtype=torch.float64).new_zeros(d.size()).cuda()\n",
    "    \n",
    "    # calculate between 0 and switch_start\n",
    "    output[mask1] = (4 * epsilon_arr[mask1] * sigma_arr_6[mask1]\n",
    "                     * (sigma_arr_6[mask1] / d[mask1]**12 - sigma_arr_6[mask1] / d[mask1]**6))\n",
    "    \n",
    "    #calculate between switch start and cutoff\n",
    "    output[mask2] = (4 * epsilon_arr[mask2] * sigma_arr_6[mask2]\n",
    "                     * (sigma_arr_6[mask2] / d[mask2]**12 - sigma_arr_6[mask2] / d[mask2]**6)\n",
    "                     * 3 * ((d[mask2] - cutoff_lj) / (cutoff_lj - switch_start_lj))**2\n",
    "                     * 2 * ((d[mask2] - cutoff_lj) / (cutoff_lj - switch_start_lj))**3)\n",
    "    \n",
    "    # sum potentials\n",
    "    output = torch.sum(output, dim=-1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason the CPU seems to be the bottleneck here, still."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing_pytorch_cuda = %timeit lj_potential_pytorch_cuda(configuration, sigma, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def lj_potential_pairwise(distance, sigma_lj, epsilon_lj):\n",
    "    if(distance <= 0 or distance > cutoff_lj):\n",
    "        return 0.\n",
    "    else:\n",
    "        inv_dist = sigma_lj / distance\n",
    "        inv_dist2 = inv_dist * inv_dist\n",
    "        inv_dist4 = inv_dist2 * inv_dist2\n",
    "        inv_dist6 = inv_dist2 * inv_dist4\n",
    "        phi_LJ = 4. * epsilon_lj * inv_dist6 * (inv_dist6 - 1.)\n",
    "        if(distance <= cutoff_lj - switch_width_lj):\n",
    "            return phi_LJ\n",
    "        else:\n",
    "            t = (distance - cutoff_lj) / switch_width_lj\n",
    "            switch = t * t * (3. + 2. * t)\n",
    "            return phi_LJ * switch\n",
    "\n",
    "@njit\n",
    "def lj_potential_numba(x):\n",
    "    output = np.zeros(len(x))\n",
    "    for i in range(len(x)):\n",
    "        potential = 0\n",
    "        for j in range(i, len(x)):\n",
    "            sigma = sigma_arr[i, j]\n",
    "            epsilon = epsilon_arr[i, j]\n",
    "            distance = np.linalg.norm(x[i, :] - x[j, :])\n",
    "            potential += lj_potential_pairwise(distance, sigma, epsilon)\n",
    "        output[i] = potential\n",
    "    return output\n",
    "\n",
    "x = np.random.uniform(0, 4, (10,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(0, 4, (100, 3))\n",
    "np.testing.assert_allclose(lj_potential_numba(x), lj_potential_loops(x, sigma_arr_temp, epsilon_arr_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing_numba = %timeit lj_potential_numba(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried @njit(parallelize=True) but it was much slower for some reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary on my System\n",
    "Ubuntu 18.04  \n",
    "i3-4170 @ 3.70GHz × 4  \n",
    "NVidia GTX 960  \n",
    "16 GB RAM    \n",
    "  \n",
    "### Results\n",
    "\n",
    "| Method | calc time per loop |  \n",
    "| --- | --- | \n",
    "| Loops| 24.1 s ± 144 m |  \n",
    "| np.apply_along axis | 40.9 s ± 197 m |  \n",
    "| apply_along_axis with loop | 13.9 s ± 180 ms |  \n",
    "| np.piecewise (no types) | 1.55 s ± 46.6 ms |  \n",
    "| np.masking | 4.85 ± 139 ms |  \n",
    "| pytorch masking | 7.45 s ± 10.2 ms |  \n",
    "| pytorch masking CUDA | 1.14 s ± 76.6 ms |  \n",
    "| Numba | 1.27 s ± 21.1 ms | \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
